---
description: LLaVA ì •ë¦¬
---

# LLaVA: Large Language and Vision Assistant

GitHub: [https://github.com/haotian-liu/LLaVA?tab=readme-ov-file](https://github.com/haotian-liu/LLaVA?tab=readme-ov-file)

ë°ëª¨ : [https://llava.hliu.cc/](https://llava.hliu.cc/)

<figure><img src="../.gitbook/assets/image.png" alt=""><figcaption><p>LLaVA ë°ëª¨</p></figcaption></figure>

## Intro

* Microsoft Research
* ê±°ëŒ€ Vision-Language ì˜¤í”ˆì†ŒìŠ¤ (ì±—ë´‡) ëª¨ë¸
* ë…¼ë¬¸ ì œëª©ì€ Visual Instruction Tuning
  * Instructionì„ GPT-4ë¡œ ë§Œë“¤ì—ˆë‹¤
* ë²„ì „ 1, 1.5, 1.6 ìˆìŒ
  * ìµœì‹  ë²„ì „ì¼ìˆ˜ë¡ ë” ì¢‹ì€ ë°ì´í„°, ìµœì í™” ê°œì„ 
*   ìƒì—…ì  ì´ìš© ë¼ì´ì„¼ìŠ¤ í™•ì¸ í•„ìš”

    * LLaMA, OpenAI, ShareGPTì˜ ë¼ì´ì„¼ìŠ¤ë¥¼ ë”°ë¦„
    *   ShareGPT: diffusionì˜ í”„ë¡¬í”„íŠ¸ ê³µìœ  ì‚¬ì´íŠ¸ ChatGPT ë²„ì „

        <figure><img src="../.gitbook/assets/image (7).png" alt=""><figcaption></figcaption></figure>
    * LLaMA: ì›”ê°„ ì‚¬ìš©ìê°€ 7ì–µëª… ì´ìƒì¼ ê²½ìš° ë¼ì´ì„¼ìŠ¤ë¥¼ ìš”ì²­?

    <figure><img src="../.gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>

## LLaVA: Visual Instruction Tuning

* text-only GPT-4ë¡œ vision-langauge instruction-following data ìƒì„±
  * Chat ê°€ëŠ¥
* End-to-endë¡œ í•™ìŠµëœ ê±°ëŒ€ vision-language ëª¨ë¸
  * Vison Encoder (OpenCLIP) + LLM (Vicuna)
  * base ëª¨ë¸, ëª¨ë¸ ê°€ì¥ ê°„ë‹¨í•œ êµ¬ì¡° ì‚¬ìš©í•¨
* GPT-4ë¡œ ìƒì„±í•œ ë°ì´í„°, í•™ìŠµëœ ëª¨ë¸, ì½”ë“œ ê³µê°œ

### (ë°ì´í„°) GPT-assisted Visual Instruction Data Generation

* ê¸°ì¡´ì˜ image-text ë°ì´í„° ì‚¬ìš©
* text-only GPT-4 ì‚¬ìš©
  * í…ìŠ¤íŠ¸ë¡œ ì´ë¯¸ì§€ í‘œí˜„í•˜ê¸° ìœ„í•´ ë°ì´í„°ì…‹ì˜ captionê³¼ bounding box ì‚¬ìš©
  *   3 ê°€ì§€ ì¢…ë¥˜ì˜ instruction (158k)\
      : conversation(58k), detailed description(23k), complex reasoning(77k)

      <figure><img src="../.gitbook/assets/image (3).png" alt=""><figcaption><p>instructional vision-langauge ë°ì´í„° ì˜ˆì‹œ</p></figcaption></figure>



### (í•™ìŠµ) Visual Instruction Tuning

#### ëª¨ë¸ êµ¬ì¡°

<figure><img src="../.gitbook/assets/image (4).png" alt=""><figcaption></figcaption></figure>

* $$X_v$$: ì´ë¯¸ì§€, $$(X_q, X_a)$$: í…ìŠ¤íŠ¸ (instruction) question-answer
* $$H_v = W \cdot Z_v, \;\;with\;\; Z_v = CLIP(X_v)$$
  * $$W$$: dimension ë§ì¶°ì£¼ëŠ” projection layer, ì—¬ê¸°ì„œëŠ” linear ì‚¬ìš© => base ëª¨ë¸
  * $$Z_v$$: ì´ë¯¸ì§€ grid features

#### í•™ìŠµ ë°ì´í„° êµ¬ì„±

* ê° ì´ë¯¸ì§€ì— ëŒ€í•´ Multi-turn data êµ¬ì„± $$X_v,$$$$(X_q^1, X_a^1, ..., X_q^T, X_a^T)$$
  * ì²« ë²ˆì§¸ turnì—ì„œë§Œ ì´ë¯¸ì§€ ì •ë³´ ì£¼ë„ë¡ ë””ìì¸
  *

      <figure><img src="../.gitbook/assets/á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-02-07 á„‹á…©á„Œá…¥á†« 5.20.27.png" alt=""><figcaption></figcaption></figure>

      <figure><img src="../.gitbook/assets/image (5).png" alt=""><figcaption><p>Multi-turn ë°ì´í„° ì˜ˆì œ</p></figcaption></figure>

#### í•™ìŠµ ë°©ë²•

*   ëª©ì  í•¨ìˆ˜ : instruction ì£¼ì–´ì¡Œì„ ë•Œ, answerì™€ \<STOP> ì˜ˆì¸¡ (ì´ˆë¡ìƒ‰ í† í°)

    <figure><img src="../.gitbook/assets/image (6).png" alt=""><figcaption></figcaption></figure>
* ë‘ ë‹¨ê³„ë¡œ í•™ìŠµ
  * Stage 1: Pre-training for Feature Alignment
    * Vision Encoderì˜ projection layer í•™ìŠµ
    * image-text ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ ì‚¬ìš©
      * $$X_q$$ : ì´ë¯¸ì§€ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”
      * $$X_a$$ : ground-truth text ë°ì´í„°
  * Stage 2: Fine-tuning End-to-End
    *      Vision Encoderì˜ projection layer, LLM í•™ìŠµ
    * GPT-4ë¡œ ìƒì„±í•œ instructional data ì‚¬ìš© => Multi-Modal Chatbot!!!!

### Limitation

* ì‚¬ì „ ì§€ì‹ì´ í•„ìš”í•œ ê²½ìš° ex. ë‹¤êµ­ì ì–¸ì–´ ì´í•´, ìŒì‹ ì„¤ëª…
* "ë”¸ê¸°, ìš”ê±°íŠ¸" ì™€ "ë”¸ê¸°ë§› ìš”ê±°íŠ¸" êµ¬ë¶„í•˜ì§€ ëª»í•¨
* ë‹¨ë‹µ ëŒ€ë‹µ ëª»í•¨&#x20;



## LLaVA 1.5: Improved Baselines with Visual Instruction Tuning

* ì˜ ì•ˆë¼? ë°ì´í„° ì¶”ê°€í•´ ğŸ‘Š
  * ë‹¨ìˆœ ì‘ë‹µì˜ VQA (Vision Question Answering) ë°ì´í„° ì¶”ê°€
    * ì§ˆë¬¸ì— ë‹¨ìˆœ ì‘ë‹µ format ì¶”ê°€ ex) Q: .... í•œ ë‹¨ì–´ë¡œ ë‹µí•˜ì‹œì˜¤.
  * ShareGPT ë°ì´í„° ì¶”ê°€ => ë‹¤êµ­ì ì–¸ì–´ ì´í•´ ê°œì„ 
* ì˜ ì•ˆë¼? í¬ê¸° ëŠ˜ë ¤ ğŸ‘Š
  * LLM 7B -> 13B
  * ì´ë¯¸ì§€ í•´ìƒë„ ë†’ì„, ë” í° CLIP Visionëª¨ë¸ê³¼ MLP projection ì‚¬ìš©
    *

        <figure><img src="../.gitbook/assets/image (8).png" alt=""><figcaption></figcaption></figure>


* Limitation
  * ëª¨ë¸ ì¦ê°€í•œ ë§Œí¼ computational cost ì¦ê°€
  * ëŠ˜ ê·¸ë ‡ë“¯ í•™ìŠµí•˜ì§€ ì•Šì€ ê±° ì—¬ì „íˆ ëª»í•¨



## LLaVA-1.6: Improved reasoning, OCR, and world knowledge

<figure><img src="../.gitbook/assets/image (10).png" alt=""><figcaption></figcaption></figure>

*   (1) Dynamic High Resolution

    * ê³ í•´ìƒë„ ì´ë¯¸ì§€ì˜ íš¨ìœ¨ì  ì²˜ë¦¬

    <figure><img src="../.gitbook/assets/image (11).png" alt=""><figcaption></figcaption></figure>
* (2) Data Mixture
  * [LAION-GPT-V](https://huggingface.co/datasets/laion/gpt4v-dataset) and [ShareGPT-4V](https://sharegpt4v.github.io/) ì‚¬ìš©
  * ì‹¤ì œ ìœ ì €ì˜ [LLaVA demo](https://llava-vl.github.io/)15K ë°ì´í„°
  * í•„í„°ë§ í›„, GPT-4Vë¡œ instructional data ìƒì„±
  * TextCaps, TextVQA ë™ì¼í•œ ì´ë¯¸ì§€ ì‚¬ìš©, TextCaps ì œê±°
  * OCR ìœ„í•´ì„œ DocVQA, SynDog-EN ì¶”ê°€
  * ì°¨íŠ¸, ë‹¤ì´ì–´ê·¸ë¨ ìœ„í•´ì„œ ChartQA, DVQA, AI2D ì¶”ê°€
*   (3) Scailing LLM Backbone

    * &#x20;[Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/) and [Nous-Hermes-2-Yi-34](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B)í…ŒìŠ¤íŠ¸

    <figure><img src="../.gitbook/assets/image (12).png" alt=""><figcaption></figcaption></figure>



